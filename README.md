# Atelier 4 : Deep Learning - Mod√®les G√©n√©ratifs (AE, VAE, GANs)

**Universit√© Abdelmalek Essaadi** **Facult√© des Sciences et Techniques de Tanger** **Master :** MBD (Big Data)  
**Module :** Deep Learning  
**Professeur :** Pr. ELAACHAK LOTFI  

---

## üìã Description
Ce projet s'inscrit dans le cadre du quatri√®me laboratoire de Deep Learning. L'objectif principal est de se familiariser avec la biblioth√®que **PyTorch** en impl√©mentant et en entra√Ænant diff√©rentes architectures de r√©seaux de neurones profonds pour l'intelligence artificielle g√©n√©rative.

Le laboratoire est divis√© en deux parties principales :
1.  **Auto-encodeurs (AE) et Auto-encodeurs Variationnels (VAE)** appliqu√©s au dataset MNIST.
2.  **R√©seaux Antagonistes G√©n√©ratifs (GANs)** appliqu√©s √† la g√©n√©ration d'art abstrait.

## üõ†Ô∏è Technologies utilis√©es
* **Langage :** Python
* **Biblioth√®ques :** PyTorch, Torchvision, Matplotlib, Numpy, Scikit-learn
* **Environnement :**  Google Colab

---

## 1Ô∏è‚É£ Partie 1 : AE et VAE (MNIST)

### üéØ Objectifs
* Mettre en place une architecture d'Auto-encodeur (AE) standard.
* Mettre en place une architecture d'Auto-encodeur Variationnel (VAE).
* Entra√Æner les mod√®les sur le dataset **MNIST** (chiffres manuscrits).
* Comparer les performances (Perte de reconstruction, Divergence KL) et visualiser l'espace latent.

### üß† Architectures
* **Auto-Encoder (AE) :**
    * *Encoder* : R√©duit la dimension de l'image (784 pixels) vers un espace latent compress√©.
    * *Decoder* : Reconstruit l'image √† partir du vecteur latent.
    * *Loss* : MSELoss (Mean Squared Error).
* **Variational Auto-Encoder (VAE) :**
    * Introduit une composante probabiliste dans l'espace latent (moyenne `mu` et variance `logvar`).
    * Utilise le "reparameterization trick" pour permettre la r√©tropropagation.
    * *Loss* : Reconstruction Loss (BCE) + KL Divergence.

### üìä R√©sultats
* Comparaison des courbes de perte entre AE et VAE.
* Visualisation de la reconstruction des chiffres.
* Analyse de l'espace latent (t-SNE ou scatter plot) montrant la s√©paration des classes de chiffres.

---

## 2Ô∏è‚É£ Partie 2 : GANs (Abstract Art Gallery)

### üéØ Objectifs
* Utiliser des GANs pour g√©n√©rer de nouvelles images artistiques.
* Dataset utilis√© : [Abstract Art Gallery](https://www.kaggle.com/datasets/bryanb/abstract-art-gallery).

### üß† Architecture GAN
* **G√©n√©rateur (Generator) :**
    * Prend en entr√©e un vecteur de bruit al√©atoire (z_dim=100).
    * Utilise des couches de **ConvTranspose2d** (d√©convolution) pour g√©n√©rer une image RGB 64x64.
    * Activation : ReLU et Tanh pour la sortie.
* **Discriminateur (Discriminator) :**
    * Prend en entr√©e une image (r√©elle ou g√©n√©r√©e).
    * Utilise des couches de **Conv2d** pour classer l'image comme "R√©elle" ou "Fausse".
    * Activation : LeakyReLU et Sigmoid pour la sortie.

### ‚öôÔ∏è Entra√Ænement
* **Optimizers :** Adam (lr=0.0002, betas=(0.5, 0.999)).
* **Fonction de co√ªt :** BCELoss (Binary Cross Entropy).
* **Strat√©gie :** Entra√Ænement altern√© o√π le g√©n√©rateur essaie de tromper le discriminateur, et le discriminateur essaie de ne pas se faire tromper.

---

## üöÄ Comment ex√©cuter
1.  Cloner ce d√©p√¥t :
    ```bash
    git clone [https://github.com/votre-username/votre-repo.git](https://github.com/votre-username/votre-repo.git)
    ```
2.  Ouvrir les notebooks (`atelier-4-partie-1.ipynb` et `atelier-4-partie-2.ipynb`) dans Jupyter, Kaggle ou Google Colab.
3.  S'assurer que l'acc√©l√©ration GPU est activ√©e pour un entra√Ænement plus rapide.
4.  Ex√©cuter les cellules s√©quentiellement.

## üìù Auteur
Travail r√©alis√© par **AZARIZ ANAS** 
